# -*- coding: utf-8 -*-
"""Random_Forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9uEe9lizBc-jPuXfnmjfsuh0llaGw4C
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import pyspark
from pyspark.sql import *
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf

# create the session
conf = SparkConf().set("spark.ui.port", "4050")

# create the context
sc = pyspark.SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

spark

import pandas as pd
import numpy as np

data = 'dataset.csv'
df = pd.read_csv(data)

df.head(5)

"""#Question 1 (Regression modeling):

## Question 1.1

###Lets first clean the data in order to produce correlation matrix and correlelogram
"""

df.columns.str.contains('bitrates')

df_new = df[df.columns[~df.columns.str.contains('bitrates')]]

df_new = df_new[df_new.columns[~df_new.columns.str.contains('videoChunkSizesHTTP')]]

df_new.shape[1]

df_new = df_new[df_new.columns[~df_new.columns.str.contains('chunkSizesPcapAudio')]]

df_new.shape[1]

df_new = df_new[df_new.columns[~df_new.columns.str.contains('audioChunkSizesHTTP')]]
df_new = df_new[df_new.columns[~df_new.columns.str.contains('videoChunkDursHTTP')]]
df_new = df_new[df_new.columns[~df_new.columns.str.contains('chunkSizesPcap')]]
df_new = df_new[df_new.columns[~df_new.columns.str.contains('chunkSizesPcapVideo')]]
df_new = df_new[df_new.columns[~df_new.columns.str.contains('audioChunkDursHTTP')]]

df_reserve = df_new

df_new.drop(['videoID', 'category', 'join_time', 'stallingNumber', 'totalStallDuration', 'numberOfVideoChunks', 'numberOfAudioChunks', 'ratioOfChunksHTTP', 'avgResQual', 'numberOfPosSwitches', 'numberOfNegSwitches', 'totalSwitches', 'dur', 'QoE_JT', 'QoE_noStall' ], axis=1, inplace=True)

df_new.shape

list(df_new.columns.values)

"""###Lets remove the rows where target variable is -1"""

df_new = df_new[df.QoE_ITU_046 != -1]

df_new.shape

corr_matrix = df_new.corr()

"""### The correlation matrix follow by correlogram"""

corr_matrix

corr_matrix.shape

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(60,50), dpi= 80)
sns.heatmap(df_new.corr(), xticklabels=df_new.corr().columns, yticklabels=df_new.corr().columns, cmap='RdYlGn', center=0, annot=True)

# Decorations
plt.title('Correlogram of All feature', fontsize=18)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()

value_feature = corr_matrix.tail(1)

value_feature = value_feature.drop(['QoE_ITU_046'], axis=1)

maxValue = value_feature.max(axis=1)
print('Maximum value in each row : ')
print(maxValue)

value_feature

"""###The feature having the highest correlation is: pcapStats_stats_avg_DL_TP which is 0.67

##Question 1.2
###Now lets convert the data into Spark Dataframe
"""

df_data = df = spark.createDataFrame(df_new)

df_data.count()

df_data.take(1)

df_data.printSchema()

data_colm = df_new.drop(['QoE_ITU_046'], axis=1)
colm_lst = list(data_colm.columns.values)
print(colm_lst)

from pyspark.ml.feature import VectorAssembler
vectorAssembler = VectorAssembler(inputCols = colm_lst, outputCol = 'features')
vhouse_df = vectorAssembler.transform(df_data)
vhouse_df = vhouse_df.select(['features', 'QoE_ITU_046'])

vhouse_df.show(3, truncate = True)

splits = vhouse_df.randomSplit([0.8, 0.2])
train_df = splits[0]
test_df = splits[1]

train_df.count()

"""###For Linear regression"""

from pyspark.ml.regression import LinearRegression
lr = LinearRegression(featuresCol = 'features', labelCol='QoE_ITU_046', maxIter=10, regParam=0.3, elasticNetParam=0.8)
lr_model = lr.fit(train_df)
print("Coefficients: " + str(lr_model.coefficients))
print("Intercept: " + str(lr_model.intercept))

trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

print("numIterations: %d" % trainingSummary.totalIterations)
print("objectiveHistory: %s" % str(trainingSummary.objectiveHistory))
trainingSummary.residuals.show(5)

train_df.describe().show()

lr_predictions = lr_model.transform(test_df)
lr_predictions.select("prediction","QoE_ITU_046","features").show(5)

test_result = lr_model.evaluate(test_df)
print("Root Mean Squared Error (RMSE) on test data = %g" % test_result.rootMeanSquaredError)

"""####RMSE on test data is: 0.683294

###For Random Forest
"""

from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.feature import VectorIndexer
from pyspark.ml.evaluation import RegressionEvaluator

featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(vhouse_df)

(trainingData, testData) = vhouse_df.randomSplit([0.8, 0.2])

# Train a RandomForest model.
rf = RandomForestRegressor(featuresCol="indexedFeatures", labelCol='QoE_ITU_046')

# Chain indexer and forest in a Pipeline
pipeline = Pipeline(stages=[featureIndexer, rf])

# Train model.  This also runs the indexer.
model = pipeline.fit(trainingData)

# Make predictions.
predictions = model.transform(testData)

# Select example rows to display.
predictions.select("prediction", "QoE_ITU_046", "features").show(5)

evaluator = RegressionEvaluator(
    labelCol="QoE_ITU_046", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

"""###RMSE on test data is: 0.517663

###RMSE is low for random forest as compare to linear regression. Random forest can learn complex boundries therefore on certain data like in this case it  give superior performance

##Question 1.4
## Using top 3 features
"""

variables = []
corr_value = []
import six
for i in df_data.columns:
    if not( isinstance(df_data.select(i).take(1)[0][0], six.string_types)):
        variables.append(i)
        corr_value.append(df_data.stat.corr('QoE_ITU_046',i))

data_tuples = list(zip(variables,corr_value))
data_frame = pd.DataFrame(data_tuples, columns=['Features','Corr_vale'])

"""###Therefore, top three features which have highest correlation with QoE_ITU_046 are shown below"""

data_frame.sort_values(by=['Corr_vale'], ascending=False).head(4)

vectorAssembler = VectorAssembler(inputCols = ['pcapStats_stats_avg_DL_TP', 'pcapStats_stats_p40_DL_TP', 'pcapStats_stats_p50_DL_TP'], outputCol = 'features')
features_df = vectorAssembler.transform(df_data)
features_df = features_df.select(['features', 'QoE_ITU_046'])

features_df.show(3, truncate = False)

"""###Again training the Random Forest Model and evaluating the performance"""

featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(features_df)

(trainingData, testData) = features_df.randomSplit([0.8, 0.2])

# Train a RandomForest model.
rf = RandomForestRegressor(featuresCol="indexedFeatures", labelCol='QoE_ITU_046')

# Chain indexer and forest in a Pipeline
pipeline = Pipeline(stages=[featureIndexer, rf])

# Train model.  This also runs the indexer.
model = pipeline.fit(trainingData)

# Make predictions.
predictions = model.transform(testData)

# Select example rows to display.
predictions.select("prediction", "QoE_ITU_046", "features").show(5)

evaluator = RegressionEvaluator(
    labelCol="QoE_ITU_046", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

"""### RMSE Trained on all input feature = 0.517
### RMSE Trained on top 3 features = 0.561

##Question 1.5

###Applying PCA
"""

vhouse_df.show(5, truncate= False)

from pyspark.ml.feature import PCA

pca = PCA(k=3, inputCol="features", outputCol="pcafeatures")
model = pca.fit(vhouse_df)

result = model.transform(vhouse_df).select(['pcafeatures', 'QoE_ITU_046'])
result.show(3, truncate=False)

pca_features = result.withColumnRenamed("pcafeatures","features")
pca_features.show(3, truncate = False)

"""###Now traing the Random Forest Model Again"""

featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(pca_features)

(trainingData, testData) = pca_features.randomSplit([0.8, 0.2])

# Train a RandomForest model.
rf = RandomForestRegressor(featuresCol="indexedFeatures", labelCol='QoE_ITU_046')

# Chain indexer and forest in a Pipeline
pipeline = Pipeline(stages=[featureIndexer, rf])

# Train model.  This also runs the indexer.
model = pipeline.fit(trainingData)

# Make predictions.
predictions = model.transform(testData)

# Select example rows to display.
predictions.select("prediction", "QoE_ITU_046", "features").show(5)

evaluator = RegressionEvaluator(
    labelCol="QoE_ITU_046", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

"""#### Model train in 1.4 have slight better accuracy compare to model train in 1.5

#### In term of interpretability model 1.4 is better. As we are using only 3 feature in that it can be determine more clearly that which feature with what certain value is the data is spliting and the model is then making a certain prediction.

#Question 2

##Question 2.1
"""

df_reserve.drop(['videoID', 'category', 'join_time', 'stallingNumber', 'totalStallDuration', 'numberOfAudioChunks', 'ratioOfChunksHTTP', 'avgResQual', 'numberOfPosSwitches', 'numberOfNegSwitches', 'totalSwitches', 'dur', 'QoE_ITU_046', ], axis=1, inplace=True)

df_reserve.head(2)

"""###Converting to Spark dataframe"""

df_class = spark.createDataFrame(df_reserve)

data_colm1 = df_reserve.drop(['QoE_JT'], axis=1)
colm_lst1 = list(data_colm1.columns.values)
print(colm_lst1)

from pyspark.ml.feature import VectorAssembler
vectorAssembler = VectorAssembler(inputCols = colm_lst1, outputCol = 'features')
vclass_df = vectorAssembler.transform(df_class)
vclass_df = vclass_df.select(['features', 'QoE_JT'])

vclass_df.show(3, truncate = True)

vclass_df = vclass_df.withColumnRenamed("QoE_JT","label")
vclass_df.show(2)

"""###Lets train the model."""

from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator


# Fit on whole dataset to include all labels in index.
labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(vclass_df)

# Automatically identify categorical features, and index them.
# Set maxCategories so features with > 4 distinct values are treated as continuous.
featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(vclass_df)

# Split the data into training and test sets (30% held out for testing)
(trainingData, testData) = vclass_df.randomSplit([0.8, 0.2])

# Train a RandomForest model.
rf = RandomForestClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures", numTrees=10)

# Convert indexed labels back to original labels.
labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel",
                               labels=labelIndexer.labels)

# Chain indexers and forest in a Pipeline
pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])

# Train model.  This also runs the indexers.
model = pipeline.fit(trainingData)

# Make predictions.
predictions = model.transform(testData)

predictions.select("predictedLabel", "label", "features").show(5)

# Select (prediction, true label) and compute test error
evaluator = MulticlassClassificationEvaluator(
    labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Test Error = %g" % (1.0 - accuracy))

gbtModel = model.stages[2]
print(gbtModel)

prediction_lst = [float(row.predictedLabel) for row in predictions.collect()]
prediction_lst = [int(i) for i in prediction_lst]

from collections import Counter
Counter(prediction_lst)

actual_label = [float(row.label) for row in predictions.collect()]
actual_label = [int(i) for i in actual_label]

Counter(actual_label)

"""###Confusion Matrix"""

from sklearn.metrics import confusion_matrix
data = confusion_matrix(actual_label, prediction_lst)

import seaborn as sn
df_cm = pd.DataFrame(data, columns=np.unique(actual_label), index = np.unique(actual_label))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (7,5))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, cmap="Blues", annot=True, fmt="d", annot_kws={"size": 12})#

"""###Double check the result"""

prediction_series = pd.Series( (v for v in prediction_lst) )
correct_predictions = np.sum(prediction_series == actual_label)

print("Numbers of correctly predicted values: ", correct_predictions)

"""### Precision = TP / (TP + FP)
### Putting the values from confusion matrix
### Precision = 1458/ (1458 + 0) = 1
"""

from sklearn.metrics import precision_score
precision_score(actual_label, prediction_lst, average='macro')

"""##Question 2.2

## ROC curve
"""

train, test = vclass_df.randomSplit([0.80, 0.20], seed = 42)
print(train.count())
print(test.count())
# first we check how LogisticRegression perform 
from pyspark.ml.classification import LogisticRegression
LR = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=15)
LR_model = LR.fit(train)

trainingSummary = LR_model.summary
roc = trainingSummary.roc.toPandas()
plt.plot(roc['FPR'],roc['TPR'])
plt.ylabel('False Positive Rate')
plt.xlabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
print('Training set ROC: ' + str(trainingSummary.areaUnderROC))

from pyspark.ml.evaluation import BinaryClassificationEvaluator

predictions_LR = LR_model.transform(test)
evaluator = BinaryClassificationEvaluator()
print("Area under the curve: " + str(evaluator.evaluate(predictions_LR, {evaluator.metricName: "areaUnderROC"})))

"""###Area under the curve = 0.9998

##Question 2.3

###Performing grid search in order to find the best paramets to enhance the performance of Random Forest model
"""

from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# Train a RandomForest model.
rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=10)
pipeline = Pipeline(stages=[rf])
paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20,30]).addGrid(rf.maxDepth, [4,8,10]).build()

crossval = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(),
                          numFolds=3) 

cvModel = crossval.fit(trainingData)

cvModel.avgMetrics

"""###Now perform the prediction. cvModel use the best model to perform the prediction"""

evaluator = MulticlassClassificationEvaluator(
    labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(new_predictions)
print("Test Error = %g" % (1.0 - accuracy))

"""###Test error on best trained model = 0.00009002

#Question 3

##Question 3.1
"""

df_panda = df["join_time"]

df_pand = df_panda/10000

"""###Calculating CDF"""

stats_df = df_pand \
.groupby('join_time') \
['join_time'] \
.agg('count') \
.pipe(pd.DataFrame) \
.rename(columns = {'join_time': 'frequency'})

# PDF
stats_df['pdf'] = stats_df['frequency'] / sum(stats_df['frequency'])

# CDF
stats_df['cdf'] = stats_df['pdf'].cumsum()
stats_df = stats_df.reset_index()
stats_df

"""###Plotting CDF for varaible join_time"""

stats_df.plot(x = 'join_time', y = ['cdf'], grid = True)

"""###From the CDF curve we can see that 92 percent of the videos have join_time less then 10 sec

##Question 3.2

###Finding unique video count per category using Spark SQL.
"""

df_plot = spark.read.format("csv").option("header", "true").load("/content/drive/MyDrive/Assignment05/datasetYouTubeNetwork.csv")

df_plot.createOrReplaceTempView("SQL_table")

category_table = spark.sql("SELECT videoID, COUNT(*) c FROM SQL_table GROUP BY videoID HAVING c > 1")

category_table.show()

category_table = spark.sql("SELECT  category, COUNT(DISTINCT videoID) as unique_Count FROM SQL_table GROUP BY category ORDER BY unique_Count desc")

"""###Therefore, unique videoID count for each category is shown below"""

category_table.show()

category_count = spark.sql("SELECT COUNT(DISTINCT category) AS Total_Categories FROM SQL_table")

category_count.show()

category_table.count()

table_category = category_table.toPandas()

"""###Finally, plotting the bar chart"""

ax = table_category.plot.barh(x='category', y='unique_Count')